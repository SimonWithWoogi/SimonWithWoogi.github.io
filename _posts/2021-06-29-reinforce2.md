---
title: Markov Decision Process, MDP
author: Simon Anderson
date: 2021-06-25 14:40:00 +0800
categories: [MATLAB, Reinforcement Learning]
tags: [BigData, AI, MATLAB, Reinforcement Learning]
image:
  src: /assets/img/RL/1_Preview.png
math: true
---



 본 내용은 `David Silver` 교수님의 `Reinforcement Learning` 강좌, `Markov Decision Process` 부분을 토대로 작성한 글입니다.



## <span style="color:darkblue">1. What is Markov Decision Process?</span>

 `Markov Decision Process(MDP)` 는 대부분 강화학습에서 사용하고 있는 만큼 꼭 알아야 합니다. [앞 장](https://simonwithwoogi.github.io/posts/reinforce1/) 에서는 `Agent` 를 주로 얘기했습니다. 사실 `environment` 가 정확히 무엇인지 설명하지 않았기도 합니다. 이번 `MDP` 를 통해서 `환경(Environment)` 에 대해 설명하겠습니다. 왜냐하면 `MDP` 는 `강화학습(Reinforcement Learning)` 에서 `환경(Environment)` 를 설명하기 때문이죠.

`MDP` 를 간단히 정의하면, `Process` 의 `온전한 특성들(completely characteries)` 이라고 할 수 있습니다.

### <span style="color:darkblue">1.1. Markov Property</span>

![생각중](/assets/img/RL/2_1.png)

 "The future is independent of the past given the present"

현재가 주어진다는 조건아래 미래는 과거로부터 독립적이다라는 얘기인데요. `과거(History)` 를 없애고 `State` 만 기억하는 것이 `Markov Property` 라고 합니다. 


$$
\begin{array}{l}
\mathbb{P}[S_{t+1}|S_t]=\mathbb{P}[S_{t+1}|S_1,...,S_t]
\end{array}
$$


 수식은 과거로부터 독립적임을 표현한 것일 뿐이고 1(과거, 혹은 시작점)에서부터 지금까지 쌓인 `State` 가 결국 현재의 `State` 와 미래에 일어날 S_{t+1}에 대해서 확률적으로 동일하다는 것을 얘기하고 있습니다. 즉, 현재 `State` 가 충분한 통계량을 가지고 있기에 미래를 예측할 수 있다고 할 수 있죠. 이 내용은 `Markov` 의 주장이니 근거를 살펴봅시다. **그 전에, 이 수식이 맞다면 우리는 무엇을 얻을 수 있을까요?** 과거의 모든 상태를 다 따져서 메모리에 넣어두기보다 현재 상태만 가지고 있어서 보다 더 빠른 계산도 가능할 것이고 자원관리 차원에서 효율도 충분히 올라갈 것으로 보입니다.

#### <span style="color:darkblue">State Transition Matrix</span>

 `State Transistion Matrix` 는 지금의 상태 s와 그로부터 이어지는 상태 s'로 이어지는 확률들을 `희소행렬(Sparse Matrix)` 의 형태로 만든 것입니다.


$$
\begin{array}{l}
P_{ss'}=\mathbb{P}[S_{t+1}=s'|S_t=s]
\end{array}
$$
Pss'는 이어지는 상태에 대해 발생할 확률을 얘기하겠죠. 여러 상태가 경우의 수로서 만들어지지 않겠습니까? 그게 아래처럼 보여질 수 있습니다.

잘 생각해보면 행의 합이 1입니다. 예를 들겠습니다. 0~9까지 적힌 10장의 카드를 하나 뽑았을 때, 0이 나왔습니다. 이어서 뽑을 때 1~9까지 나올 확률은 100%(1)고 각 값들이 나올 확률은 100/9 %(1/9) 이니까요.

![STM](/assets/img/RL/2_2.png)

### <span style="color:darkblue">1.2. Markov Chains(Markov Process)</span>

 `State Transition Matrix` 에서 연쇄적인 상태에 대해 언급이 됐습니다. 그 생각이 발전되어 `Markov Chains(Markov Process)` 는 연쇄적인 상태발생에 대한 얘기를 합니다.

#### <span style="color:darkblue">Markov Process</span>

 먼저 `Markov Process` 는 복원추출개념의 `Memoryless Random Process`를 얘기합니다. 수학적 정의을 먼저 볼까요? 혹시 아시는 분이 있을 지는 모르겠는데, `Timed-Finite State Automata(F-FSA)` 와 유사한 성질을 지닙니다. 학술적으로는 다르지만, 아시는 분이 있다면 `State` 는 동일하고 `Time` 값이 `Prob.` 값으로 바뀌었으니 쉽게 이해할 수 있습니다.
$$
\begin{array}{l}
\text{Markov Process is a tuple <S, P>}\\
S \text{ is a (finite) set of states}\\
P \text{ is a state transition (probability) matrix},\\
P_{ss'}=\mathbb{P}[S_{t+1}=s'|S_t=s]
\end{array}
$$
 수학적 정의를 보면 `Markov Process` 는 `2-tuple` 구조입니다. S는 유한한 상태의 집합이고, P는 위에서 설명한  `STM` 입니다. 아래는 `Markov Process Diagram` 입니다.

![TFSA](/assets/img/RL/2_3.png)

![STM](/assets/img/RL/2_4.png)

이에 대한 `확률 희소행렬(P)` 입니다. 길게 설명하진 않겠지만, 상태와 확률에 대해서 트리 혹은 행렬로 표현합니다. 이제 좀 `환경(Environment)` 라고 할 수 있겠죠?

## <span style="color:darkblue">2. Markov 'Reward' Process</span>

![곁들인](/assets/img/RL/2_5.png)

그동안 `Markov Process` 를 설명했습니다. 이제 `보상(Rward)` 를 곁들인...
$$
\begin{array}{l}
\text{Markov Reward Process is a tuple <S, P, R,} \gamma>\\
S \text{ is a (finite) set of states}\\
P \text{ is a state transition (probability) matrix},\\
P_{ss'}=\mathbb{P}[S_{t+1}=s'|S_t=s]\\
R \text{ is a reward function, }R_s=\mathbb{E}[R_{t+1}|S_t=s]\\
\gamma \text{ is a discount factor, }\gamma \in[0,1]
\end{array}
$$
 `Markov Reward Process` 는 4-tuple로 구성됩니다. 추가된 R은 `기대되는 보상(Reward)` 그 자체를 의미하고, `감마` 는 미래에 얻을 수 있는 `보상(Reward)` 의 중요도를 조절할 수 있는 `Discount factor` 입니다. `강화학습(Reinforcement)` 는 `목표 지향 계산법(Goal-Directed Computational Approach)` 이기에 보상은 `agent` 를 움질일 수 있도록 도와주죠. 참고로 `감마` 가 0에 가까울 수록 `근시적 평가(myopic)` 가 이뤄지고 1에 가까울수록 `원시적 평가(far-sighted)` 가 이뤄집니다.


$$
\begin{array}{l}
G_t = R_{t+1} + \gamma R_{t+2}+...=\Sigma_{k=0}^\infty \gamma^k R_{t+k+1}
\end{array}
$$
 `감마` 위에 `k` 가 있으니, **스텝이 이어질수록 점점 보상이 적어집니다.**
$$
\text{State value function  v(s)}\\
\begin{array}{l}
\text{v(s)}=\mathbb{E}[G_t|S_t=s]
\end{array}
$$


**왜 Discount factor가 필요한가?**

---

- 수학적으로 가중치(`Discount factor`)는 계산(`Reward`)을 편하게 도와줍니다.
- `Markov Process` 는 순환가능합니다. Class1 -> Feedback 을 영원히 돌 수 있죠. 그러니, 순환에서 이탈하도록 도와줍니다.
- 불확실한 미래에 대한 감안을 할 수 있습니다.
- 보상이 재정적이라면, 즉시 보상(`immediate rewards`)에 대해 더 높은 가치를 부여합니다.
- 행동은 즉각적인 보상에 대한 선호도를 가지게 합니다.

---

![MRP Diagram](/assets/img/RL/2_6.png)

 이제 슬슬 다이어그램이 제법 강화학습에서 얘기하는 `환경(Environment)` 로 보이네요. `Reward` 를 통해 어디를 가야할 지 목표가 있어 보입니다.

값을 대입하면 아래와 같은 모양이 나옵니다. 단, 쉬운 계산을 위해 `감마` 는 0입니다.

![MRP Diagram gamze](/assets/img/RL/2_6.png)

## <span style="color:darkblue">2.1. Bellman Equation</span>

`Richard E. Bellman` 은 저희에게 친숙한 이름입니다. `Bellan-Ford algorithm` 에서 동적 계획법을 통해 미래가치를 보고 최단거리를 찾는 아이디어를 제안한 사람이고, `차원의 저주`라는 표현을 맨 처음 얘기한 사람입니다. 

 `State Value function` 은 두 가지의 조건으로 나눠집니다.
$$
\begin{array}{l}
\text{즉시 보상(Immediate reward)}\ R_{t+1}\\
\text{미래 가치에 대한 값}\ \gamma \text{v}(S_{t+1})\\
\text{v(s)} = \mathbb{E}[G_t|S_t=s]\\
= \mathbb{E}[R_{t+1}+\gamma R_{t+2} +\gamma^2R_{t+3}+...|S_t=s]\\
= \mathbb{E}[R_{t+1}+\gamma(R_{t+2} +\gamma R_{t+3}+...)|S_t=s]\\
= \mathbb{E}[R_{t+1}+\gamma G_{t+1}|S_t=s]\\
= \mathbb{E}[R_{t+1}+\gamma\text{v}(S_{t+1})|S_t=s]\\
\end{array}
$$
 ![Bellan eg](/assets/img/RL/2_7.png)

현재 가치에 미래 가치를 더하여 최적의 경로를 찾아내는 것이죠.

결론적으로 `Bellan equation` 은 선형방정식이라 아래 표현처럼 볼 수 있습니다.
$$
\begin{array}{l}
\text{v} = R+\gamma P\text{v}\\

\begin{bmatrix}
\text{v(1)}\\
\vdots \\
\text{v(n)}
\end{bmatrix}
=
\begin{bmatrix}
R_1\\
\vdots \\
R_n\end{bmatrix}
+ \gamma
\begin{bmatrix}
P_{11} \cdots P_{1n}\\
\vdots \\
P_{n1} \cdots P_{nn}
\end{bmatrix}
\begin{bmatrix}
\text{v(1)}\\
\vdots \\
\text{v(n)}
\end{bmatrix}
\end{array}
$$
자 그럼 방정식을 풀어보겠습니다.
$$
\begin{array}{l}
\text{v} = R+\gamma P\text{v}\\
(I-\gamma P)\text{v} = R\\
\text{v} = (I-\gamma P)^{-1}R
\end{array}
$$
`Big-O notation` 에 의한 계산복잡도는 `O(n^3)` 이 나옵니다. 이처럼 규모가 작을 때는 바로 해결할 수 있고 규모가 클 때는 아래 세 가지의 이미 알려진 방법으로 해결할 수 있습니다.

---

- Dynamic programming
- Monte-Carlo Simulation
- Temporal-Difference learning

---



## <span style="color:darkblue">3. Markov 'Decision' Process</span>



## <span style="color:darkblue">4. Extensions to MDPs</span>

