---
title: Reinforcement Learning, 강화학습이란 무엇인가
author: Simon Anderson
date: 2021-06-25 14:40:00 +0800
categories: [MATLAB, Reinforcement Learning]
tags: [BigData, AI, MATLAB, Reinforcement Learning]
image:
  src: /assets/img/RL/1_Preview.png
math: true
---



## <span style="color:darkblue">1. What is Reinforcement Learning?</span>

![Phd](/assets/img/RL/1_1.png)

**??? : "제법 쓸만한 모델이 나올지도? 후후.."**

 `강화학습(Reinforcement Learning)` 은 `목표 지향 계산법(Goal-Directed Computational Approach)`입니다. 불확실한 환경과 상호작용을 통해 컴퓨터는 주어진 `업무(task)` 를 학습합니다. 이러한 학습방식은 `누적된 보상(Cumulative reward)` 의 최대화를 통해 결정에 관련된 문제들을 해결할 수 있습니다. 더 나아가 인간의 간섭과 전통적인 기준치에 의한 프로그램에서 벗어나 업무를 진행할 수 있죠.

### <span style="color:darkblue">1.1. The goal of reinforcement learning</span>

![structure](/assets/img/RL/1_2.png)

 `강화학습` 의 목표는 `불확실한 환경(Unknown environment)` 속에서 `agent` 를 학습하여 `업무(task)` 를 완벽하게 수행하는 것입니다. `agent` 는 환경 속 데이터(`observations`)과 `보상(reward)` 를 받습니다. 그리고 `행동(action)` 을 환경에 보내줍니다. `보상(reward)` 는 업무 목적에 얼마나 가까운 행동을 수행했는 지에 대한 정량적 결과입니다.

 학술용어인 `agent` 는 사실 `행위자` 라는 이름으로 사용되는데, `entity` , `unit`, `worker` 등으로 볼 수 있습니다. 비슷한 이유로 `observations` 또한 센서 데이터를 받아오는 것, 환경에 대한 로그 데이터 등의 모니터링 값입니다. 한글로 번역된 단어들은 좋은 표현이 아니므로 `agent`, `observation(s)` 는 번역하여 설명하지 않습니다.

 `agent` 는 두 가지의 주요 특성을 포함하고 있습니다. 바로 `정책(policy)와 학습 알고리즘(learning algorithm)` 입니다.

---

**정책(Policy):** 마치 퍼즐 혹은 짝을 맞춘다는 느낌의 `매핑(mapping)` 입니다. `환경(environment)` 에서 제공하는 `observations` 를 기반으로 `행동(actions)` 을 선택하는 과정이죠. 보편적으로 `정책(policy)` 는 조정가능한 파라미터를 통해 **함수 근사기(function approximator)** 로 동작합니다. **함수 근사기는 1.3에 설명 하겠습니다.** 예를 들면, 딥러닝 네트워크가 있죠.

**학습 알고리즘(Learning Algorithm):**`행동(action), observations, 보상(reward)` 을 기반으로 `정책(Policy)` 속 파라미터를 지속적으로 업데이트합니다. `학습 알고리즘(Learning Algorithm)` 의 목표는 `업무(task)` 속에서 `누적된 보상(cumulative reward)` 을 최대화하는 `정책(Policy)` 를 찾는 겁니다.

---

 어렵게 얘기했지만, 결국 `강화학습(Reinforcement Learning)` 의 목표는 반복적인 시도를 통해 최적의 행동을 학습하는 것입니다. 이제 여기에 인간의 개입도 없고 반복적인 시행착오가 들어가는 것이죠.

### <span style="color:darkblue">1.2. Examples</span>

 자동 주차를 예로 들어보겠습니다. 여기서 목표는 올바른 위치, 방향으로 주차하는 것입니다. `agent` 가 핸들도 돌리고, 가속 및 제동, 시동을 끄고 여러 `행동(actions)` 을 하겠죠. 이런 상황에서는 카메라, 가속도계, 자이로센서, GPS, 라이다 등등 여러 센서들이 들어가고 이것이 `observations` 입니다. `observations` 의 데이터 말고도, 차량의 무게같은 정보와, 지면 재질, 여러 역학적 요소들이 `환경(environment)`를 구성합니다. 

### <span style="color:darkblue">1.3. Function approximator</span>

![fa](/assets/img/RL/1_3.png)

 위 그림처럼 실제 데이터(녹색)를 설명하고자 한다면, 아마 무수히 많은 좌표들을 저장해야할 것입니다. 차라리 이 데이터를 적어도 비슷하게 설명할 수 있는 함수로서 구성한다면 데이터를 모두 저장하는 것보다 효율적인 방식이 될 수 있겠죠. 지금 위 실제 데이터(녹색)는 17 포인트이며 x, y를 모두 저장해야 합니다. 만약 이 데이터가 6차원에 100,000 포인트를 넘어서 존재한다면 컴퓨터 리소스는 부담이 될 수 있습니다. 100000x6 짜리 행렬이 나올테니까요.

 그러나 `Function approximator(빨간 선)` 를 이용한다면, 데이터를 저장할 필요없이 `y = 2x` 라는 함수로서 실제 데이터(녹색)과 근사하게 값을 얻을 수 있습니다. 단순한 1차 함수를 표현했지만, 만약 7차 함수여도 `y=ax^7+bx^6+cx^5+dx^4+ex^3+fx^2+gx+h` 형태로서 `offset` 역할을 하는 `h`를 포함하여 `a부터 h` 총, 8개의 파라미터만 집어넣어도 데이터를 알 수 있죠.

 `강화학습(Reinforcement Learning)` 에서는 `Function approximator` 가 `정책(Policy)` 에 동작하고, 딥러닝 네트워크를 예로 들었습니다. 이는 주기적인 업데이트를 통해 실제 데이터를 잘 반영하는 함수를 찾는 것으로 이해할 수 있는데, 지금은 `Function approximator` 에 대한 개념만 알아두시고 `벨만 방정식, DQN` 등을 설명하고나서 본격적으로 다루겠습니다.

## <span style="color:darkblue">2. Reinforcement Learning process</span>

![process](/assets/img/RL/1_4.png)

 `강화학습(Reinforcement Learning)` 만이 아니라 어떤 모델을 설계해도 대부분 위와 같은 절차를 따를 것입니다. 다만 `용어(Terminology)` 의 차이일 뿐이니, 어렵게 생각하지 않으셔도 됩니다. 기본적으로 `문제정의 - 초기화 - 구현 - 검증 - 반영`을 따릅니다.

---

`Formulate problem`: 문제 정의 단계입니다. 여기서는 `agent` 가 `환경(Environment)` 이랑 어떻게 상호작용할 것인지 방법을 정의하고 `agent` 가 달성할 목표는 무엇인지, 무슨 `업무(task)` 를 통해 학습할 것인지 정의합니다.

`Crete environment`: 환경 생성 단계입니다. `agent` 와 인터페이스할 `환경과 환경동적모델(Environment dynamic model)` 을 정의합니다.

`Define reward`: 보상 정의 단계입니다. `agent` 의 행동과 목표를 비교하여 성과를 측정하고 어떻게 보상할 것인지 정의합니다.

`Create agent`: `정책(Policy)` 정의하고 학습 알고리즘을 구성된 `agent` 를 만듭니다.

`Train agent`: 정의된 `환경(Environment)` 와 `보상(Reward)` 그리고 `학습알고리즘(Learning algorithm)` 을 통해 `정책(Policy)` 을 업데이트 합니다.

`Validation agent`: 지금까지 시뮬레이션 했던 `agent` 의 성능을 평가하는 단계입니다.

`Deploy policy`: 교육된 `정책(Policy)` 를 배포하는 단계입니다.

---

