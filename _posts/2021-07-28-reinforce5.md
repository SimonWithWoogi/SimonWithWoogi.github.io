---
title: Dynamic programming, 강화학습에서의 동적 프로그래밍
author: Simon Anderson
date: 2021-07-28 20:44:00 +0800
categories: [MATLAB, Reinforcement Learning]
tags: [BigData, AI, MATLAB, Reinforcement Learning]
image:
  src: /assets/img/RL/1_Preview.png
math: true
---

 이번 포스팅은 [MDP](https://simonwithwoogi.github.io/posts/reinforce2/) 에 대한 간략한 설명과 [Grid World](https://simonwithwoogi.github.io/posts/reinforce3/)를 실습하면서 어느정도 프레임이 잡혔다고 생각합니다. 그럼 세밀하게 들어가야 된다는 판단하에, `Grid World` 를 통해 `동적(다이나믹) 프로그래밍(Dynamic programming)` 을 알아보겠습니다.

## <span style="color:darkblue">1. What is the Dynamic in Computer Science</span>

​	이번 포스팅에서는 `강화학습(Reinforcement Learning)`에서의 `동적 프로그래밍(Dynamic programming)` 을 설명할텐데, 이번 챕터에서는 컴퓨터에서 적용하는 `Dynamic`  이라는 개념을 얘기하고자 합니다. 이 챕터가 지나가면 지금 메인 주제와 상관없는 `동적 할당(Dynamic memory allocation)` 마저도 개념이 자연스레 들어올 것입니다. 물론 기반을 다지는 것이지 `동적할당` 이 무엇인지는 모르신다면 읽어보셔야 합니다.

​	`프로그램(Program)` 은 `알고리즘(Algorithm)+자료구조(Data Structure)` 이라고 부릅니다. 그래서 컴퓨터 속의 프로그램은 `Memory` 속에서 어떤 `해결방법(Solution)`을 구성요소로 가지고 있는 것이라 볼 수 있죠. 보통 여기서 `Dynamic method` 는 풀어야할 본래의 `해결방법(Solution)` 의 규모보다 작은 규모로서 일반화를 통해 해결하는 것을 얘기합니다. 비슷한 개념으로는 `분할정복(Divide and Conquer)` 이 있습니다. 저는 주관적으로 `동적 프로그래밍(Dynamic programming)` 이랑 `분할정복(Divide and Conquer)` 를 명백히 다르다고 봅니다만, 아래 두 비교를 통해서 여러분들은 엄밀한 의미에서 같은 뿌리다라고 하실 수 있습니다. 

|      | Dynamic Programming                                          | Divide and Conquer                                           |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 개념 | 작은 문제들을 해결한 다음 해결한 결과를 통해<br />앞으로의 큰 문제들을 해결하는 방식 | 직면한 문제를 나눌 수 없을만큼 최대한 나눈 다음<br />각각을 풀어나가는 방식 |
| 방향 | Bottom-up(상향식)                                            | Top-down(하향식)                                             |
| 예시 | TSP, 피보나치 수열                                           | Binary Searching, Quick Sorting                              |

​	이제 제가 왜 주관적으로 다르다고 얘기하지만, 반대로 같다고 주장하는 분들도 왜 존중하는지 `TSP(Traveling Salesman Problem)` 와 `Binary Searching` 으로 비교하겠습니다.

### <span style="color:darkblue">1.1. Traveling Salesman Problem</span>

 [이전 포스팅](https://simonwithwoogi.github.io/posts/matlabastaralgo/) `A* algorithm` 을 이용하여 `최단거리` 에 접근한 내용입니다.

![img](/assets/img/MATLAB/1_3.png)
![img](/assets/img/MATLAB/1_4.png)

​	문제는 이렇습니다. `Arad` 에서 시작하여 `Bucharest` 로 도착하는 가장 빠른 길은 무엇일지 따지는 것입니다. `TSP(Traveling Salesman Problem)` 는 모든 도시에 대한 이동 비용들이 주어진다면 **최소비용으로 모든 도시를 순회하는 방법**에서 시작됐습니다. 그런데 위 문제는 모든 도시를 순회하는 것이 아니라 **시작 도시에서 끝 도시까지의 최소 거리**를 구하고 있습니다. 이번 기회에 `TSP` 와 `최단거리 알고리즘(Shorttest algorithm)` 유사함을 보여드리며 `Dynamic Programming` 에 왜 `Dynamic` 이 들어가는지 설명하겠습니다.`TSP` 도 `최단거리` 에서의 문제 모두 도시와 길을 추가할 때마다 많은 경우의 수가 생깁니다. 행정안전부가 제공하는 대한민국 도시는 162개이며 현재시간의 도로상황을 가중치로 두어서 서울에서 대전까지 가장 빨리 갈 수 있는 길과 서울에서 대전을 점으로 두어 타원을 만들었을 때 안에 들어오는 모든 도시를 최단 시간으로 한번씩 순회하는 문제라고 볼 수 있습니다. 두 문제 모두 똑같이 풀어나갈 수 있습니다. 

![채워나가기](/assets/img/RL/4_1.png)

​	어차피 정답은 정해져 있습니다. 최소비용, 최단거리, 최소시간은 저희가 구성할 수 있는 모든 조합중에 반드시 유일하진 않습니다만 어쨌든 존재하기 때문이죠. 그러니 첫 발부터 내딛어야 합니다. 방대한 문제를 메모리에서 치워버리고, 시작점에서부터 어떤 방향으로 나아갈지 결정합니다. 어떤 방향에 대한 기준은 여러 알고리즘마다 다르죠. 그리고 그 시작점의 다음, 그리고 그 다음 이렇게 누적된 비용들을 계산하여 비교합니다.

![가려진조합들](/assets/img/RL/4_2.png)

​	막상 보면 인간이 길을 찾을 때 하는 사고방식과 유사합니다. 목표지점이 어딨는지 대충 알고있으니 방향을 유지한 채로 오른쪽 왼쪽 길을 결정하며 나아갑니다. 해당 문제의 모든 조합들이 있지만, 그런건 다 가려진 채로 당장 다음에 어떤 방향으로 갈 것인가에 대한 고민만 하는 것이죠. 그게 반복하다보면 목표지점에 도달합니다. 출발하기 전부터 경로를 뚝딱 만들어서 비용을 계산하는 것이 아니라, 비용을 계산하면서 경로를 만들어냅니다. 만약 다 만들어진 경로의 길이가 40개의 도시이며 메모리가 320 바이트라고 했을때, 처음에는 8 바이트, 16 바이트, ..., 320 바이트로 늘어납니다. 추가적으로 비용을 계산하는 과정에서 도와주던 여러 변수들도 생겼다가 사라지다보니 메모리가 쉴 틈이 없습니다. 늘었다가 줄었다가 하거든요. 반대로 경로를 미리 만들어서 총 비용을 한 번에 계산하는 방식은 처음부터 끝까지 320 바이트를 유지합니다. 

### <span style="color:darkblue">1.2. Binary Searching</span>

![한심좌가르기](/assets/img/RL/4_3.png)

​	일단 쪼갭니다. 가상의 환경을 만들겠습니다.  [0 2 3 4 6 7 9 10 11 17 18 24] 이라는 벡터가 있고 여기서 10이라는 숫자를 찾는다고 하겠습니다. 이때 `Binary Searching` 은 아래와 같은 절차를 거쳐 10을 찾아냅니다.

![이진탐색절차](/assets/img/RL/4_4.png)

가운데를 찍고 가운데 수가 10보다 작으니 작은 영역(좌측)을 다 날려버립니다.

9 10 11 17 18 24

또 마찬가지로 가운데를 찍고 10보다 크니, 우측을 다날려줍니다.

9 10

9? 10보다 크니 날려버립니다.

10을 찾았군요.

​	만약 벡터의 길이가 40이고, 메모리의 크기가 320 바이트라고 했을때, `Binary Searching` 을 한번씩 할 때마다, 메모리의 크기는 절반씩 줄어들게 됩니다. 결국 큰 문제를 작은 문제를 어떻게 나누냐의 차이라서 컴퓨터의 입장에선 메모리가 동적으로 변하는 것은 `Dynamic Programming` 와 `Divide and Conquer` 모두 해당하죠. 인간의 사고시점에서 본다면, 메모리가 점점 늘어나고 줄어드는 차이로 `상향식(Bottom-up), 하향식(Top-down)`의 차이가 있는 것이고 컴퓨터의 시점에서 본다면 하나의 문제에 대해 메모리가 변화하는 것은 모두 `Dynamic` 하다고 볼 수 있습니다.

## <span style="color:darkblue">2. Dynamic Programming in Basic Grid World</span>

​	이제 본격적으로, `강화학습(Reinforcement Learning)` 에서 얘기하는 `동적 프로그래밍(Dynamic Programming)` 을 설명하겠습니다. 사실 `벨만 방정식(Bellman Equation)` 을 정의한 `리차드 벨만(Richard Ernest Bellman)` 이 `동적 프로그래밍` 을 만들었습니다.

### <span style="color:darkblue">2.1. Recall concepts</span>

​	중요한 몇 부분만 다시 복습하겠습니다.

#### <span style="color:darkblue">Value function</span>

​	`가치함수(Value function)` 은 `Agent` 가 좋은 `정책(Polices)` 을 판단하는 정량적 방법입니다. 그러므로 `가치함수` 에는 현재 상태 St 로부터 미래에 기대되는 보상까지 설명하고 있습니다.
$$
v_\pi(s) = \mathbb{E}_\pi[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s]
$$
​	이에 나란히하는 `Q function` 이 있습니다. 이는 `Agent` 가 선택할 행동에 대한 직접적인 가치를 알려주는 함수입니다.
$$
q_\pi(s, a)=\mathbb{E}_\pi[R_{t+1}+\gamma q_\pi(S_{t+1},A_{t+1})|S_t=s, A_t=a]
$$

#### <span style="color:darkblue">Bellman Equation</span>

​	`벨만 방정식(Bellman Equation)` 은 현재 가치와 미래 가치를 더한 개념입니다. 그 중에서도 `벨만 기대 방정식(Bellman Expectation Equation)` 이 위에 수식으로 표현한 가치함수입니다.
$$
v_\pi(s) = \mathbb{E}_\pi[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s]
$$
​	기대값을 알 수 있으니, 여러 반복을 통해 `최적의 정책(Optimal Policy)`을 찾을 것입니다. 결론적으로 최적의 가치를 받게하는 것이 최적의 정책이고 이를 아래처럼 표현할 수 있으며 `벨만 최적 방정식(Bellman Optimality Equation)` 이라고 부릅니다.


$$
v_*(s)=\ \max_\pi\ v_\pi(s)\qquad q_*(s, a)=\max_\pi\ q_\pi(s, a)\\
v_*(s)=\max_{a}\mathbb{E}[R_{t+1}+\gamma v_*(S_{t+1})|S_t=s, A_t=a]\\
q_*(s,a)=\max_{s,a}\mathbb{E}[R_{t+1}+\gamma q_*(S_{t+1}, A_{t+1})|S_t=s, A_t=a]
$$

### <span style="color:darkblue">2.2. Dynamic Programming</span>

​	위에서 얘기했지만, `동적 프로그래밍` 은 작은 문제들을 해결하여 큰 문제를 해결하는 방식입니다. 이를 `강화학습` 에 적용하여 다시 얘기한다면 `벨만 기대 방정식` 을 반복적으로 풀어나감으로서 `벨만 최적 방정식`을 `동적 프로그래밍` 으로 찾습니다. 그리고 모든 강화학습은 순차적으로 진행합니다. 마치 위에서 최단거리를 찾기위해 한 도시마다 방문했던 것처럼요. 저 문제에서는 한번에 두 개의 도시를 방문할 수 없습니다. 

본격적으로 `Grid World` 에 대해서 `동적 프로그래밍` 을 보겠습니다. [전 포스팅](https://simonwithwoogi.github.io/posts/reinforce3/) 에서 동일한 `Grid World` 를 가져오겠습니다.

 ![Grid World](/assets/img/RL/3_2.png)

`Grid World Environment` 는 아래 조건들을 따르고 있습니다.

---

- 5x5 로 구성된 맵과 동,서,남,북으로 움질일 수 있다.
- `Agent` 는 2행 1열에서 출발한다.
- `Agent` 가 끝지점`(terminal state)` 에 도달하면 +10의 보상을 받는다.
- [2,4]는 순간이동 구간`(special jump)`이라 [4,4]로 바로 이동할 수 있으며 +5의 보상을 받는다.
- 검정으로된 구간은 장애물 구간으로 막혀있다. [3,3] [3,4] [3,5], [4,3]
- 그 외에 모든 행동들은 -1의 패널티가 주어진다.

---

 ![RLAlgoFlow](/assets/img/RL/4_5.png)

​	대부분의 `강화학습(Reinforcement Learning)` 교재에서 위 절차를 안내하고 있습니다. 이번 시간에는 `Dynamic Programming` 을 적용하는 `MDP` 를 추려 보겠습니다.

