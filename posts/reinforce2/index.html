<!DOCTYPE html><html lang="ko-KR" mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.0" /><meta property="og:title" content="Markov Decision Process, MDP" /><meta name="author" content="Simon Anderson" /><meta property="og:locale" content="ko_KR" /><meta name="description" content="본 내용은 David Silver 교수님의 Reinforcement Learning 강좌, Markov Decision Process 부분을 토대로 작성한 글입니다." /><meta property="og:description" content="본 내용은 David Silver 교수님의 Reinforcement Learning 강좌, Markov Decision Process 부분을 토대로 작성한 글입니다." /><link rel="canonical" href="https://simonwithwoogi.github.io/posts/reinforce2/" /><meta property="og:url" content="https://simonwithwoogi.github.io/posts/reinforce2/" /><meta property="og:site_name" content="몬기의 기술공방" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-06-25T15:40:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Markov Decision Process, MDP" /><meta name="twitter:site" content="@no-name" /><meta name="twitter:creator" content="@Simon Anderson" /><meta name="google-site-verification" content="Simon Anderson verification" /> <script type="application/ld+json"> {"author":{"@type":"Person","name":"Simon Anderson"},"description":"본 내용은 David Silver 교수님의 Reinforcement Learning 강좌, Markov Decision Process 부분을 토대로 작성한 글입니다.","url":"https://simonwithwoogi.github.io/posts/reinforce2/","@type":"BlogPosting","headline":"Markov Decision Process, MDP","dateModified":"2021-08-23T12:35:43+09:00","datePublished":"2021-06-25T15:40:00+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://simonwithwoogi.github.io/posts/reinforce2/"},"@context":"https://schema.org"}</script><title>Markov Decision Process, MDP | 몬기의 기술공방</title><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/Blog_logo_210213.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">몬기의 기술공방</a></div><div class="site-subtitle font-italic">Simon with Woogi</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/SimonWithWoogi" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/no-name" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['simon.anderson.tech','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>Markov Decision Process, MDP</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Markov Decision Process, MDP</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Simon Anderson </span> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Fri, Jun 25, 2021, 3:40 PM +0900" prep="on" > Jun 25 <i class="unloaded">2021-06-25T15:40:00+09:00</i> </span></div><div> <span> <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Mon, Aug 23, 2021, 12:35 PM +0900" prefix="Updated " > Aug 23 <i class="unloaded">2021-08-23T12:35:43+09:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="4314 words">23 min</span></div></div><div class="post-content"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/RL/1_Preview.png" class="preview-img" alt="Preview Image"><p>본 내용은 <code class="language-plaintext highlighter-rouge">David Silver</code> 교수님의 <code class="language-plaintext highlighter-rouge">Reinforcement Learning</code> 강좌, <code class="language-plaintext highlighter-rouge">Markov Decision Process</code> 부분을 토대로 작성한 글입니다.</p><h2 id="1-what-is-markov-decision-process"><span style="color:darkblue">1. What is Markov Decision Process?</span></h2><p><code class="language-plaintext highlighter-rouge">Markov Decision Process(MDP)</code> 는 대부분 강화학습에서 사용하고 있는 만큼 꼭 알아야 합니다. <a href="https://simonwithwoogi.github.io/posts/reinforce1/">앞 장</a> 에서는 <code class="language-plaintext highlighter-rouge">Agent</code> 를 주로 얘기했습니다. 사실 <code class="language-plaintext highlighter-rouge">environment</code> 가 정확히 무엇인지 설명하지 않았기도 합니다. 이번 <code class="language-plaintext highlighter-rouge">MDP</code> 를 통해서 <code class="language-plaintext highlighter-rouge">환경(Environment)</code> 에 대해 설명하겠습니다. 왜냐하면 <code class="language-plaintext highlighter-rouge">MDP</code> 는 <code class="language-plaintext highlighter-rouge">강화학습(Reinforcement Learning)</code> 에서 <code class="language-plaintext highlighter-rouge">환경(Environment)</code> 를 설명하기 때문이죠.</p><p><code class="language-plaintext highlighter-rouge">MDP</code> 를 간단히 정의하면, <code class="language-plaintext highlighter-rouge">Process</code> 의 <code class="language-plaintext highlighter-rouge">온전한 특성들(completely characteries)</code> 이라고 할 수 있습니다.</p><h3 id="11-markov-property"><span style="color:darkblue">1.1. Markov Property</span></h3><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/RL/2_1.png" alt="생각중" /></p><p>“The future is independent of the past given the present”</p><p>현재가 주어진다는 조건아래 미래는 과거로부터 독립적이다라는 얘기인데요. <code class="language-plaintext highlighter-rouge">과거(History)</code> 를 없애고 <code class="language-plaintext highlighter-rouge">State</code> 만 기억하는 것이 <code class="language-plaintext highlighter-rouge">Markov Property</code> 라고 합니다.</p>\[\begin{array}{l} \mathbb{P}[S_{t+1}|S_t]=\mathbb{P}[S_{t+1}|S_1,...,S_t] \end{array}\]<p>수식은 과거로부터 독립적임을 표현한 것일 뿐이고 1(과거, 혹은 시작점)에서부터 지금까지 쌓인 <code class="language-plaintext highlighter-rouge">State</code> 가 결국 현재의 <code class="language-plaintext highlighter-rouge">State</code> 와 미래에 일어날 S_{t+1}에 대해서 확률적으로 동일하다는 것을 얘기하고 있습니다. 즉, 현재 <code class="language-plaintext highlighter-rouge">State</code> 가 충분한 통계량을 가지고 있기에 미래를 예측할 수 있다고 할 수 있죠. 이 내용은 <code class="language-plaintext highlighter-rouge">Markov</code> 의 주장이니 근거를 살펴봅시다. <strong>그 전에, 이 수식이 맞다면 우리는 무엇을 얻을 수 있을까요?</strong> 과거의 모든 상태를 다 따져서 메모리에 넣어두기보다 현재 상태만 가지고 있어서 보다 더 빠른 계산도 가능할 것이고 자원관리 차원에서 효율도 충분히 올라갈 것으로 보입니다.</p><h4 id="state-transition-matrix"><span style="color:darkblue">State Transition Matrix</span></h4><p><code class="language-plaintext highlighter-rouge">State Transistion Matrix</code> 는 지금의 상태 s와 그로부터 이어지는 상태 s’로 이어지는 확률들을 <code class="language-plaintext highlighter-rouge">희소행렬(Sparse Matrix)</code> 의 형태로 만든 것입니다.</p>\[\begin{array}{l} P_{ss'}=\mathbb{P}[S_{t+1}=s'|S_t=s] \end{array}\]<p>Pss’는 이어지는 상태에 대해 발생할 확률을 얘기하겠죠. 여러 상태가 경우의 수로서 만들어지지 않겠습니까? 그게 아래처럼 보여질 수 있습니다.</p>\[\quad \qquad \quad\text{to}\\P = \text{from} \left[ \begin{matrix} P_{11} \dots P_{1n}\\ \vdots\\ P_{n1} \dots P_{nn} \end{matrix} \right]\]<p>잘 생각해보면 행의 합이 1입니다. 예를 들겠습니다. 0~9까지 적힌 10장의 카드를 하나 뽑았을 때, 0이 나왔습니다. 이어서 뽑을 때 1~9까지 나올 확률은 100%(1)고 각 값들이 나올 확률은 100/9 %(1/9) 입니다.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/RL/2_2.png" alt="STM" /></p><p>다시 그림을 보여드리겠지만, 값이 들어간 희소행렬은 위와 같습니다.</p><h3 id="12-markov-chainsmarkov-process"><span style="color:darkblue">1.2. Markov Chains(Markov Process)</span></h3><p><code class="language-plaintext highlighter-rouge">State Transition Matrix</code> 에서 연쇄적인 상태에 대해 언급이 됐습니다. 그 생각이 발전되어 <code class="language-plaintext highlighter-rouge">Markov Chains(Markov Process)</code> 는 연쇄적인 상태발생에 대한 얘기를 합니다.</p><h4 id="markov-process"><span style="color:darkblue">Markov Process</span></h4><p>먼저 <code class="language-plaintext highlighter-rouge">Markov Process</code> 는 복원추출개념의 <code class="language-plaintext highlighter-rouge">Memoryless Random Process</code>를 얘기합니다. 수학적 정의을 먼저 볼까요? 혹시 아시는 분이 있을 지는 모르겠는데, <code class="language-plaintext highlighter-rouge">Timed-Finite State Automata(F-FSA)</code> 와 유사한 성질을 지닙니다. 학술적으로는 다르지만, 아시는 분이 있다면 <code class="language-plaintext highlighter-rouge">State</code> 는 동일하고 <code class="language-plaintext highlighter-rouge">Time</code> 값이 <code class="language-plaintext highlighter-rouge">Prob.</code> 값으로 바뀌었으니 쉽게 이해할 수 있습니다.</p>\[\begin{array}{l} \text{Markov Process is a tuple &lt;S, P&gt;}\\ S \text{ is a (finite) set of states}\\ P \text{ is a state transition (probability) matrix},\\ P_{ss'}=\mathbb{P}[S_{t+1}=s'|S_t=s] \end{array}\]<p>수학적 정의를 보면 <code class="language-plaintext highlighter-rouge">Markov Process</code> 는 <code class="language-plaintext highlighter-rouge">2-tuple</code> 구조입니다. S는 유한한 상태의 집합이고, P는 위에서 설명한 <code class="language-plaintext highlighter-rouge">STM</code> 입니다. 아래는 <code class="language-plaintext highlighter-rouge">Markov Process Diagram</code> 입니다.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/RL/2_4.png" alt="STM" /></p><p>상태와 확률에 대해서 트리와 행렬로 표현합니다. 이제 좀 <code class="language-plaintext highlighter-rouge">환경(Environment)</code> 이라고 할 수 있겠죠?</p><h2 id="2-markov-reward-process"><span style="color:darkblue">2. Markov ‘Reward’ Process</span></h2><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/RL/2_5.png" alt="곁들인" /></p><p>그동안 <code class="language-plaintext highlighter-rouge">Markov Process</code> 를 설명했습니다. 이제 <code class="language-plaintext highlighter-rouge">보상(Rward)</code> 를 곁들인…</p>\[\begin{array}{l} \text{Markov Reward Process is a tuple &lt;S, P, R,} \gamma&gt;\\ S \text{ is a (finite) set of states}\\ P \text{ is a state transition (probability) matrix},\\ P_{ss'}=\mathbb{P}[S_{t+1}=s'|S_t=s]\\ R \text{ is a reward function, }R_s=\mathbb{E}[R_{t+1}|S_t=s]\\ \gamma \text{ is a discount factor, }\gamma \in[0,1] \end{array}\]<p><code class="language-plaintext highlighter-rouge">Markov Reward Process</code> 는 4-tuple로 구성됩니다. 추가된 R은 <code class="language-plaintext highlighter-rouge">기대되는 보상(Reward)</code> 그 자체를 의미하고, <code class="language-plaintext highlighter-rouge">감마</code> 는 미래에 얻을 수 있는 <code class="language-plaintext highlighter-rouge">보상(Reward)</code> 의 중요도를 조절할 수 있는 <code class="language-plaintext highlighter-rouge">Discount factor</code> 입니다. <code class="language-plaintext highlighter-rouge">강화학습(Reinforcement)</code> 는 <code class="language-plaintext highlighter-rouge">목표 지향 계산법(Goal-Directed Computational Approach)</code> 이기에 보상은 <code class="language-plaintext highlighter-rouge">agent</code> 를 움질일 수 있도록 도와주죠. 참고로 <code class="language-plaintext highlighter-rouge">감마</code> 가 0에 가까울 수록 <code class="language-plaintext highlighter-rouge">근시적 평가(myopic)</code> 가 이뤄지고 1에 가까울수록 <code class="language-plaintext highlighter-rouge">원시적 평가(far-sighted)</code> 가 이뤄집니다.</p>\[G_t = R_{t+1} + \gamma R_{t+2}+...=\sum_{k=0}^\infty \gamma^k R_{t+k+1}\]<p><code class="language-plaintext highlighter-rouge">감마</code> 위에 <code class="language-plaintext highlighter-rouge">k</code> 가 있으니, <strong>스텝이 이어질수록 점점 보상이 적어집니다.</strong></p>\[\text{State value function v(s)}\\ \begin{array}{l} \text{v(s)}=\mathbb{E}[G_t|S_t=s] \end{array}\]<p><strong>왜 Discount factor가 필요한가?</strong></p><hr /><ul><li>수학적으로 가중치(<code class="language-plaintext highlighter-rouge">Discount factor</code>)는 계산(<code class="language-plaintext highlighter-rouge">Reward</code>)을 편하게 도와줍니다.<li><code class="language-plaintext highlighter-rouge">Markov Process</code> 는 순환가능합니다. Class1 -&gt; Facebook 을 영원히 돌 수 있죠. 그러니, 순환에서 이탈하도록 도와줍니다.<li>불확실한 미래에 대한 감안을 할 수 있습니다.<li>보상이 재정적이라면, 즉시 보상(<code class="language-plaintext highlighter-rouge">immediate rewards</code>)에 대해 더 높은 가치를 부여합니다.<li>행동은 즉각적인 보상에 대한 선호도를 가지게 합니다.</ul><hr /><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/RL/2_6.png" alt="MRP Diagram" /></p><p>이제 슬슬 다이어그램이 제법 강화학습에서 얘기하는 <code class="language-plaintext highlighter-rouge">환경(Environment)</code> 로 보이네요. <code class="language-plaintext highlighter-rouge">Reward</code> 를 통해 어디를 가야할 지 목표가 있어 보입니다.</p><p>값을 대입하면 아래와 같은 모양이 나옵니다. 단, 쉬운 계산을 위해 <code class="language-plaintext highlighter-rouge">감마</code> 는 0입니다.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/RL/2_7.png" alt="MRP Diagram gamze" /></p><h3 id="21-bellman-equation"><span style="color:darkblue">2.1. Bellman Equation</span></h3><p><code class="language-plaintext highlighter-rouge">Richard E. Bellman</code> 은 저희에게 친숙한 이름입니다. <code class="language-plaintext highlighter-rouge">Bellan-Ford algorithm</code> 에서 동적 계획법을 통해 미래가치를 보고 최단거리를 찾는 아이디어를 제안한 사람이고, <code class="language-plaintext highlighter-rouge">차원의 저주</code>라는 표현을 맨 처음 얘기한 사람입니다.</p><p><code class="language-plaintext highlighter-rouge">State Value function</code> 은 두 가지의 조건으로 나눠집니다.</p>\[\begin{array}{l} \text{즉시 보상(Immediate reward)}\ R_{t+1}\\ \text{미래 가치에 대한 값}\ \gamma \text{v}(S_{t+1})\\ \text{v(s)} = \mathbb{E}[G_t|S_t=s]\\ = \mathbb{E}[R_{t+1}+\gamma R_{t+2} +\gamma^2R_{t+3}+...|S_t=s]\\ = \mathbb{E}[R_{t+1}+\gamma(R_{t+2} +\gamma R_{t+3}+...)|S_t=s]\\ = \mathbb{E}[R_{t+1}+\gamma G_{t+1}|S_t=s]\\ = \mathbb{E}[R_{t+1}+\gamma\text{v}(S_{t+1})|S_t=s]\\ \end{array}\]<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/RL/2_8.png" alt="Bellan eg" /></p><p>현재 가치에 미래 가치를 더하여 최적의 경로를 찾아내는 것이죠.</p><p>결론적으로 <code class="language-plaintext highlighter-rouge">Bellan equation</code> 은 선형방정식이라 아래 표현처럼 볼 수 있습니다.</p>\[\begin{array}{l} \text{v} = R+\gamma P\text{v}\\ \begin{bmatrix} \text{v(1)}\\ \vdots \\ \text{v(n)} \end{bmatrix} = \begin{bmatrix} R_1\\ \vdots \\ R_n\end{bmatrix} + \gamma \begin{bmatrix} P_{11} \cdots P_{1n}\\ \vdots \\ P_{n1} \cdots P_{nn} \end{bmatrix} \begin{bmatrix} \text{v(1)}\\ \vdots \\ \text{v(n)} \end{bmatrix} \end{array}\]<p>자 그럼 방정식을 풀어보겠습니다.</p>\[\begin{array}{l} \text{v} = R+\gamma P\text{v}\\ (I-\gamma P)\text{v} = R\\ \text{v} = (I-\gamma P)^{-1}R \end{array}\]<p><code class="language-plaintext highlighter-rouge">Big-O notation</code> 에 의한 계산복잡도는 <code class="language-plaintext highlighter-rouge">O(n^3)</code> 이 나옵니다. 이처럼 규모가 작을 때는 바로 해결할 수 있고 규모가 클 때는 아래 세 가지의 이미 알려진 방법으로 해결할 수 있습니다.</p><hr /><ul><li>Dynamic programming(동적 계획법)<li>Monte-Carlo evaluation(몬테 카를로 평가법)<li>Temporal-Difference learning(시간차 학습)</ul><hr /><h2 id="3-markovdecision-process"><span style="color:darkblue">3. Markov ’Decision’ Process</span></h2><p>긴 시간을 함께 해주셨습니다. <code class="language-plaintext highlighter-rouge">Markov Process</code> 는 그저 맵일 뿐, 어디가 중요한지 몰랐죠. <code class="language-plaintext highlighter-rouge">Markov Reward Process</code> 는 어디가 중요한지 <code class="language-plaintext highlighter-rouge">보상(Reward)</code> 이랑 <code class="language-plaintext highlighter-rouge">Discount factor</code> 를 가중치로서 알려줬습니다. 더 나아가, 이제 <code class="language-plaintext highlighter-rouge">결정(Decision)</code> 에 대한 <code class="language-plaintext highlighter-rouge">Markov Process</code> 를 알아보겠습니다.</p><p><code class="language-plaintext highlighter-rouge">Markov Decision Process(MDP)</code> 는 <code class="language-plaintext highlighter-rouge">Markov Reward Process</code> 에 여러 결정들이 들어가 있는 경우입니다. 어떻게 결정을 할까요? 바로 <code class="language-plaintext highlighter-rouge">행동(action)</code> 이 추가됩니다. 모든 <code class="language-plaintext highlighter-rouge">States</code> 에 대한 <code class="language-plaintext highlighter-rouge">환경(Environment)</code>이라고 할 수 있죠. <code class="language-plaintext highlighter-rouge">MDP</code> 는 5-tuple 입니다.</p>\[\begin{array}{l} \text{A Mrkov Decision Process is a tuple } &lt;S, A, P, R, \gamma &gt; \\ S\text{ is a finitie set of states} \\ A\text{ is a finite set of actions} \\ P\text{ is a state transition probability matrix,} \\ P_{ss'}^a=\mathbb{P}[S_{t+1}=s'|S_t=s, A_t=a]\\ R\text{ is a reward function, }R_s^a=\mathbb{E}[R_{t+1}|S_t=s,A_t=a]\\ \gamma \text{ is a discount factor } \gamma \in [0, 1] \end{array}\]<p><code class="language-plaintext highlighter-rouge">Markov Decision Process</code> 에서는 <code class="language-plaintext highlighter-rouge">A set</code> 이 추가되고 <code class="language-plaintext highlighter-rouge">P</code> 가 변했습니다. <code class="language-plaintext highlighter-rouge">A</code> 는 유한한 수를 가지는 행동에 대한 집합이고, <code class="language-plaintext highlighter-rouge">P</code> 위에 붙은 <code class="language-plaintext highlighter-rouge">a</code> 는 다음 상태가 되기까지 현재의 <code class="language-plaintext highlighter-rouge">state</code> 와 <code class="language-plaintext highlighter-rouge">action</code> 을 고려한 확률입니다.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/RL/2_9.png" alt="MDP" /></p><h3 id="31-policies"><span style="color:darkblue">3.1. Policies</span></h3><p>제가 <a href="https://simonwithwoogi.github.io/posts/reinforce1/">앞 전</a>에 <code class="language-plaintext highlighter-rouge">정책(Policy)</code> 은 <code class="language-plaintext highlighter-rouge">Function approximator</code> 라고 설명드렸었는데요. <code class="language-plaintext highlighter-rouge">MDP</code> 를 설명하면 좀 더 개념을 잡겠습니다.</p>\[\pi (a|s)=\mathbb{P}[A_t=a|S_t=s]\]<p><code class="language-plaintext highlighter-rouge">정책(Policy)</code>은 <code class="language-plaintext highlighter-rouge">agent</code> 의 <code class="language-plaintext highlighter-rouge">행동(action)</code> 에 대해 모두 정의합니다.</p><p>MDP 정책은 <code class="language-plaintext highlighter-rouge">과거(history)</code> 를 제외하고 현재 상태에만 의존합니다. 이는 <code class="language-plaintext highlighter-rouge">Markov Process</code> 에서 현재 <code class="language-plaintext highlighter-rouge">상태(State)</code> 가 과거의 기록을 다 반영하고 있음을 따라갑니다.</p><p><code class="language-plaintext highlighter-rouge">정책(Policy)</code> 은 시간에 독립적이며 업데이트를 진행하기 전 까지는 <code class="language-plaintext highlighter-rouge">고정되어 있습니다.(stationary)</code></p>\[A_t \sim \pi(\cdot|S_t), \forall t&gt;0\]<p><code class="language-plaintext highlighter-rouge">환경(MDP)</code> 와 <code class="language-plaintext highlighter-rouge">정책(Policy)</code> 이 있다면, 과거로부터 이어져오는 일련의 <code class="language-plaintext highlighter-rouge">상태(States)</code> 는 <code class="language-plaintext highlighter-rouge">Markov Process</code> 에 있습니다. 그리고 상태와 보상이 이어지는 것은 <code class="language-plaintext highlighter-rouge">Markov Reward Process</code> 에 있죠. 이를 정리하면 아래와 같습니다.</p>\[\text{Given an MDP } M=&lt;S,A,P,R,\gamma&gt; \text{ and a policy }\pi \\ \text{The state sequence } S_1, S_2, ... \text{is a Markov Process} &lt;S,P^\pi&gt;\\ \text{The state and reward sequence } S_1, R_2, S_2, ... \text{is a Markov Reward Process}\\ \text{where}\\ P_{s,s'}^\pi = \sum_{a\in A} \pi(a|s)P^a_{ss'} \\ R_s^\pi=\sum_{a \in A}\pi(a|s)R_s^a\]<p>이제 저희는 파이가 정책이라는 점을 알았습니다. 그럼 해당 정책에서 <code class="language-plaintext highlighter-rouge">State Value function</code> 은 위 <code class="language-plaintext highlighter-rouge">Bellan Equation</code> 에서 크게 변하지 않고 아래처럼 볼 수 있습니다.</p>\[\begin{array}{l} \text{v}_\pi\text{(s)} = \mathbb{E}_\pi[G_t | S_t=s] \end{array}\]<p>또한 <code class="language-plaintext highlighter-rouge">Action-Value function</code> 은 해당 정책(파이)를 따를 때, 상태와 행동에서 기대되는 가치를 확인할 수 있습니다.</p>\[\begin{array}{l} q_{\pi}(s,a)=\mathbb{E}_{\pi}[G_t | S_t =s,\ A_t=a] \end{array}\]<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/RL/2_10.png" alt="MDP" /></p><h3 id="32-bellman-expectation-equation"><span style="color:darkblue">3.2. Bellman Expectation Equation</span></h3><p><code class="language-plaintext highlighter-rouge">State-Value function</code> 은 즉각적인 <code class="language-plaintext highlighter-rouge">reward</code>와 다음 <code class="language-plaintext highlighter-rouge">state</code> 에 <code class="language-plaintext highlighter-rouge">감마(discount factor)</code> 로 나눌 수 있습니다. <code class="language-plaintext highlighter-rouge">Bellman Equation</code> 이 다시 나오는 순간이죠.</p>\[\begin{array}{l} \text{v}_\pi(s)=\mathbb{E}_{\pi}[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s] \end{array}\]<p><code class="language-plaintext highlighter-rouge">Action-Value function</code> 도 위와 유사하게 나눌 수 있습니다.</p>\[q_{\pi}(s, a)=\mathbb{E}_\pi[R_{t+1}+\gamma q_\pi(S_{t+1},\ A_{t+1})|S_t=s, A_t=a]\]<p>이제 트리로서 설명하겠습니다.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/RL/2_11.png" alt="BEE" /></p><p><code class="language-plaintext highlighter-rouge">정책(Policy)</code>은 여전히 파이입니다. 흰색 원은 <code class="language-plaintext highlighter-rouge">상태(State)</code> 이고, 검은색 원은 <code class="language-plaintext highlighter-rouge">행동(Action)</code> 입니다. 왼쪽 이미지와 오른쪽 이미지 모두 <code class="language-plaintext highlighter-rouge">State</code> 와 <code class="language-plaintext highlighter-rouge">Action</code> 의 시작점만 다를 뿐 <code class="language-plaintext highlighter-rouge">가치 함수</code> 를 얘기하고 있습니다.</p><p>왼쪽 오른쪽 트리에 대한 수식은 이렇게 정리할 수 있겠네요.</p>\[v_{\pi}(s) =\sum_{a\in A}\pi(a|s)q_\pi(s,a)\qquad q_\pi(s,a)=R_s^a+\gamma\sum_{s'\in S}P_{ss'}^a v_\pi(s')\]<p>이 두가지는 앞으로 <code class="language-plaintext highlighter-rouge">Q function</code> , <code class="language-plaintext highlighter-rouge">Q Learning</code>, <code class="language-plaintext highlighter-rouge">DQN</code>, <code class="language-plaintext highlighter-rouge">PPO</code>, <code class="language-plaintext highlighter-rouge">SAC</code> , <code class="language-plaintext highlighter-rouge">DDPG</code> 등을 이해하는데 기저가 됩니다. <code class="language-plaintext highlighter-rouge">Action</code> 과 <code class="language-plaintext highlighter-rouge">State</code> 가 어떻게 엮여있는지 나타내줍니다.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/RL/2_12.png" alt="BEE2" /></p><p>위 그림은 <code class="language-plaintext highlighter-rouge">현재상태(s)</code> 에서 <code class="language-plaintext highlighter-rouge">다음상태(s')</code> 까지의 함수를 나타냅니다. <code class="language-plaintext highlighter-rouge">하나의 상태(s)</code>에 대해서 두 가지의 <code class="language-plaintext highlighter-rouge">action</code> 을 할 수 있고, 각 <code class="language-plaintext highlighter-rouge">action</code> 들이 두 개씩 다음 <code class="language-plaintext highlighter-rouge">state</code> 를 맡고 있습니다. 같은 이유로 오른쪽은 <code class="language-plaintext highlighter-rouge">action</code> 에 초점을 맞춘 트리입니다. 이 또한 두 가지 식으로 얘기할 수 있죠. 위 식에서 뻗어져나온 것이니 크게 다를 건 없습니다.</p>\[v_\pi(s)=\sum_{a \in A}\pi(a|s) \left(R_s^a+\gamma \sum_{s' \in S} P_{ss'}^av_\pi(s') \right) \qquad q_\pi(s,a)=R_s^a+\gamma \sum_{s'\in S}P_{ss'}^a\sum_{a' \in A} \pi(a'|s')q_\pi(s', a')\]<p><code class="language-plaintext highlighter-rouge">다음 상태(s')와 액션(a')</code> 에 대한 <code class="language-plaintext highlighter-rouge">가치 함수(function)</code> 수식들 입니다. 특히 <code class="language-plaintext highlighter-rouge">q(s,a)</code> 가 많이 바꼈죠.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/RL/2_13.png" alt="Diagram" /></p><p><code class="language-plaintext highlighter-rouge">Matrix form</code> 에 대한 <code class="language-plaintext highlighter-rouge">Bellman Expectation Equation</code> 은 아래와 같습니다.</p>\[v_\pi=R^\pi+\gamma P^\pi v_\pi \\ v_\pi = (I-\gamma P^\pi)^{-1}R^\pi\]<h3 id="33-optimal-value-function-and-policy"><span style="color:darkblue">3.3. Optimal Value Function and Policy</span></h3><p><code class="language-plaintext highlighter-rouge">최적화된 가치 함수(Optimal Value Function)</code> 는 모든 <code class="language-plaintext highlighter-rouge">정책(Policies)</code> 을 넘어서 <code class="language-plaintext highlighter-rouge">가치함수(Value function</code> 가 최대화될 때를 의미합니다. 당연하겠지만, <code class="language-plaintext highlighter-rouge">state-value function</code>, <code class="language-plaintext highlighter-rouge">action-value function</code> 둘 다 있습니다. 별거 아니지만, 아래 식을 참고해볼까요.</p>\[v_*(s)=\ \max_\pi\ v_\pi(s)\qquad q_*(s, a)=\max_\pi\ q_\pi(s, a)\]<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/RL/2_14.png" alt="OVF1" /></p><p>또한 <code class="language-plaintext highlighter-rouge">최적화된 정책(Optimal Policy)</code> 도 있습니다. <code class="language-plaintext highlighter-rouge">Optimal Policy</code> 는 <code class="language-plaintext highlighter-rouge">정책들(Policies)</code> 에서 <code class="language-plaintext highlighter-rouge">부분순서(partial ordering)</code> 를 정의합니다. 다만, 여기서는 집합론의 의미로서 수식을 표현합니다.</p>\[\pi \ge \pi'\ \text{if}\ v_\pi(s)\ \ge\ v_{\pi'}(s), \forall s\]<p>수식에 대한 설명은 아래와 같습니다.</p><hr /><p><strong>For any Markov Decision Process</strong></p><ul><li>모든 정책들 중에서, 최적의 pi*가 존재합니다.<li>모든 최적의 정책들이 최적의 가치 함수를 만들어 줍니다.<li>마찬가지로 모든 최적의 정책들이 최적의 <code class="language-plaintext highlighter-rouge">action-value function</code> 을 만들어 줍니다.</ul><p><strong>직역이 아닙니다. 아래 원 내용을 참고하세요.</strong></p><hr /><p>\(\begin{array}{l} \text{For any Markov Decision Process}\\ \cdot\ \text{There exists an optimal policy } \pi_*\ \text{that is better than or equal to all other policies, }\pi_*\ \ge\ \pi,\ \forall\pi \\ \cdot\ \text{All optimal policies achieve the optimal value function, }v_{\pi_*}(s)=v_*(s)\\ \cdot\ \text{All optimal policies achieve the optimal action-value function, }q_{\pi_*}(s,a) = q_*(s,a) \end{array}\) <code class="language-plaintext highlighter-rouge">최적의 정책(optimal policy)</code> 는 <code class="language-plaintext highlighter-rouge">최적의 q function</code> 을 최대화하면서 찾을 수 있습니다.</p>\[\pi_*(a|s)= \begin{cases} 1, &amp; \mbox{if }a=\arg\max_{a\in A} q_*(s,a)\\ 0, &amp; \mbox{otherwise} \end{cases}\]<p>어떤 <code class="language-plaintext highlighter-rouge">MDP</code> 에 대해 늘 결정적입니다. 또한 우리가 최적의 <code class="language-plaintext highlighter-rouge">q function</code> 을 알고있다면, 즉시 <code class="language-plaintext highlighter-rouge">최적의 정책(optimal policy)</code> 를 찾을 수 있습니다.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/RL/2_15.png" alt="OVF23" /></p><p><code class="language-plaintext highlighter-rouge">Sleep(끝)</code> 에서 역순으로 <code class="language-plaintext highlighter-rouge">Reward</code> 를 대입합니다. 결국 미래가치를 알 수 있게 됩니다.</p><h2 id="4-extensions-to-mdps"><span style="color:darkblue">4. Extensions to MDPs</span></h2><p><code class="language-plaintext highlighter-rouge">MDP</code> 를 응용한 몇 가지 방법을 설명하겠습니다.</p><h3 id="41-infinite-and-continuous-mdps"><span style="color:darkblue">4.1. Infinite and continuous MDPs</span></h3><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/RL/2_16.png" alt="NFLLQRDifferential" /></p><p><code class="language-plaintext highlighter-rouge">Infinite and Countinuous MDPs</code> 는 <code class="language-plaintext highlighter-rouge">연속된 시간과 상태</code>, 그리고 무한한 <code class="language-plaintext highlighter-rouge">상태(state)와 액션(action) spaces</code>를 의미합니다. <code class="language-plaintext highlighter-rouge">space</code> 는 실행 가능한 경우들로 이해하시면 됩니다. 첫 번째로 <code class="language-plaintext highlighter-rouge">무한한(Infinite) 공간(state and/or action spaces)</code> 이라면 무엇이 달라질까요? 바로 <code class="language-plaintext highlighter-rouge">Straightforward</code> 의 성질이 생깁니다. <code class="language-plaintext highlighter-rouge">깊이우선탐색(Depth-First Search)</code> 에서 어떤 노드의 자식들이 끝도 없이 이어져있다면, 옆에 있는 형제노드를 쳐다도 안보고 앞만 달려가겠죠? <code class="language-plaintext highlighter-rouge">직진성(Straightforward)</code> 입니다. 두 번째는 <code class="language-plaintext highlighter-rouge">연속된 공간(Continuous state and/or action spaces)</code> 입니다. 먼저 사람의 생각과 지금까지의 <code class="language-plaintext highlighter-rouge">MDP</code> 는 모두 <code class="language-plaintext highlighter-rouge">이산(Discrete)</code> 형태로서 동작합니다. <code class="language-plaintext highlighter-rouge">state</code> , <code class="language-plaintext highlighter-rouge">action</code> 모두가 0에서 바로 1로 바뀌듯 비연속적이죠. 만약 연속으로 바뀐다면 <code class="language-plaintext highlighter-rouge">닫힌 형태(Feed back, Closed loop, Closed form)</code>를 가지게 됩니다. 이 부분의 자세한 설명은 <strong>UC Berkeley 교수의 Benjamin Recht의 글인 <a href="http://www.argmin.net/2018/02/01/control-tour/">Total Control</a></strong> 에 매우 정확한 설명이 있습니다. 끝으로 위 두 설명에 의해 종속되는 <code class="language-plaintext highlighter-rouge">연속된 시간(Continuous time)</code> 의 얘기입니다. 연속된 시간으로 바뀐만큼 이제 <code class="language-plaintext highlighter-rouge">상태(state)</code>, <code class="language-plaintext highlighter-rouge">행동(action)</code> 에 대해 편미분을 해야합니다. 우리는 연속에 대한 변화량을 알아야하니까요.</p><h3 id="42-partially-observable-mdps"><span style="color:darkblue">4.2. Partially observable MDPs</span></h3><p><code class="language-plaintext highlighter-rouge">POMDPs(Partially Observable Markov Decision Process)</code> 는 <code class="language-plaintext highlighter-rouge">숨겨진 상태(Hidden states)</code>가 들어간 <code class="language-plaintext highlighter-rouge">MDP</code> 입니다. 7-tuple로 표현하고 아래와 같습니다.</p>\[\begin{array}{l} \text{A POMDP is a tuple } &lt;S, A, O, P, R, Z,\gamma &gt; \\ S\text{ is a finitie set of states} \\ A\text{ is a finite set of actions} \\ O\text{ is a finite set of observations}\\ P\text{ is a state transition probability matrix,} \\ P_{ss'}^a=\mathbb{P}[S_{t+1}=s'|S_t=s, A_t=a]\\ R\text{ is a reward function, }R_s^a=\mathbb{E}[R_{t+1}|S_t=s,A_t=a]\\ Z\text{ is an observation function,}\\ Z_{s'o}^a=\mathbb{P}[O_{t+1}=o|S_{t+1}=s', A_t=a]\\ \gamma \text{ is a discount factor } \gamma \in [0, 1] \end{array}\]<p><code class="language-plaintext highlighter-rouge">O는 Observation</code> 으로 환경에서 얻어지는 측정값이고 다음 상태와, 현재 행동이 주어지면 다음에 기대되는 <code class="language-plaintext highlighter-rouge">Observation</code> 확률을 알 수 있다는 <code class="language-plaintext highlighter-rouge">Z</code> 함수 입니다. 이를 보면 행동을 통해서 측정값이 나온다는 것을 표현했다고 볼 수 있죠.</p><h3 id="43-undiscounted-average-reward-mdps"><span style="color:darkblue">4.3. Undiscounted, average reward MDPs</span></h3><p>끝으로 <code class="language-plaintext highlighter-rouge">Average Reward MDP</code> 입니다. ` Ergodic Markov Process(EMP)<code class="language-plaintext highlighter-rouge"> 라고 하며 이는 사실 </code>중심극한의 정리<code class="language-plaintext highlighter-rouge">와 비슷한 얘기입니다. </code>EMP`의 설명을 아래서 확인할 수 있습니다.</p><hr /><p>순환(Recurrent): 각 상태는 무한한 숫자로 반복된다.</p><p>간헐(Aperiodic): 각 상태는 일정한 주기에 상관없다.</p><hr /><p><code class="language-plaintext highlighter-rouge">EMP</code> 는 제한적으로 고정된 분산 d^pi(s)를 가지고 있습니다. 또한 어떤 정책 파이여도 <code class="language-plaintext highlighter-rouge">Ergodic MDP</code> 는 시작 <code class="language-plaintext highlighter-rouge">상태(state)</code> 와 독립적으로 시간(p^pi)으로 나눠진 평균 <code class="language-plaintext highlighter-rouge">보상(Reward)</code>을 가집니다. 주기가 없이 반복되는 수가 엄청나게 많아지면 특정한 분포를 따르기 때문이죠.</p>\[d^\pi(s) = \sum_{s' \in S} d^{\pi}(s')P_{s's}\\ p^\pi=\lim_{T\rightarrow\infty}\frac{1}{T}\mathbb{E}\left[\sum_{t=1}^{T}R_t\right]\]<p>이러한 특성 때문에 <code class="language-plaintext highlighter-rouge">Average Reward Value Function</code> 은 <code class="language-plaintext highlighter-rouge">undiscounted</code> 합니다. 그래서 <code class="language-plaintext highlighter-rouge">Ergodic MDP</code>는 <code class="language-plaintext highlighter-rouge">평균 보상(Average Reward)</code> 으로 표현합니다. 아래는 s에서 시작할때 추가되는 보상에 대한 설명입니다. 그리고 그에 맞춘 <code class="language-plaintext highlighter-rouge">Bellan equation</code> 입니다.</p>\[\tilde{v}_{\pi}(s)=\mathbb{E}_\pi\left[\sum_{k=1}^\infty(R_{t+k}-\rho^{\pi})|S_t=s\right]\\ \\ \begin{matrix} \tilde{v}_{\pi}&amp;=&amp;\mathbb{E}_\pi\left[(R_{t+1}-\rho^\pi)+\sum_{k=1}^\infty(R_{t_k+1}-\rho^\pi)|S_t=s\right]\\ &amp;=&amp; \mathbb{E}_\pi\left[(R_{t+1}-\rho^\pi)+\tilde{v}_\pi(S_{t+1})|S_t=s\right] \end{matrix}\]</div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/matlab/'>MATLAB</a>, <a href='/categories/reinforcement-learning/'>Reinforcement Learning</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/bigdata/" class="post-tag no-text-decoration" >BigData</a> <a href="/tags/ai/" class="post-tag no-text-decoration" >AI</a> <a href="/tags/matlab/" class="post-tag no-text-decoration" >MATLAB</a> <a href="/tags/reinforcement-learning/" class="post-tag no-text-decoration" >Reinforcement Learning</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Markov Decision Process, MDP - 몬기의 기술공방&url=https://simonwithwoogi.github.io/posts/reinforce2/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Markov Decision Process, MDP - 몬기의 기술공방&u=https://simonwithwoogi.github.io/posts/reinforce2/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Markov Decision Process, MDP - 몬기의 기술공방&url=https://simonwithwoogi.github.io/posts/reinforce2/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/reinforce4/">MATLAB, 강화학습을 이용한 급수 시스템 스케쥴링 실습</a><li><a href="/posts/reinforce2/">Markov Decision Process, MDP</a><li><a href="/posts/NoticeToStudent/">Notice, 정리하는 시간</a><li><a href="/posts/projecthyundaithree/">Twitter Naver YouTube Crawler, 현대자동차 여론을 조사해보자(3)</a><li><a href="/posts/Cpp_Trim/">C++_string_Trim</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/matlab/">MATLAB</a> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/java/">java</a> <a class="post-tag" href="/tags/algorithm/">Algorithm</a> <a class="post-tag" href="/tags/deeplearning/">DeepLearning</a> <a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/statistics/">Statistics</a> <a class="post-tag" href="/tags/bigdata/">BigData</a> <a class="post-tag" href="/tags/improvement/">Improvement</a> <a class="post-tag" href="/tags/optimization/">Optimization</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/reinforce1/"><div class="card-body"> <span class="timeago small" > Jun 25 <i class="unloaded">2021-06-25T15:40:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Reinforcement Learning, 강화학습이란 무엇인가</h3><div class="text-muted small"><p> 1. What is Reinforcement Learning? ??? : “제법 쓸만한 모델이 나올지도? 후후..” 강화학습(Reinforcement Learning) 은 목표 지향 계산법(Goal-Directed Computational Approach)입니다. 불확실한 환경과 상호작용을 통해 컴퓨터는 주어진 업무(task) 를 학습합니다. ...</p></div></div></a></div><div class="card"> <a href="/posts/reinforce3/"><div class="card-body"> <span class="timeago small" > Jul 26 <i class="unloaded">2021-07-26T16:32:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>MATLAB, MDP에서 강화학습 실습</h3><div class="text-muted small"><p> 이번 포스팅은 MATLAB 을 이용한 강화학습(Reinforcement Learning) 실습입니다. 제가 예전에 RLCode 팀(Reinforcement Learning Code Team) 의 코드를 천천히 봤는데, 거기서도 이론과 실제 구현하는 두 수준의 간극이 차이가 난다고 얘기했습니다. 저도 마찬가지인데요. 그래서 MATLAB, Python, ...</p></div></div></a></div><div class="card"> <a href="/posts/reinforce4/"><div class="card-body"> <span class="timeago small" > Aug 22 <i class="unloaded">2021-08-22T21:45:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>MATLAB, 강화학습을 이용한 급수 시스템 스케쥴링 실습</h3><div class="text-muted small"><p> 이번 포스팅은 간단한 조합 문제를 강화학습으로 풀어봅시다. 해당 내용은 MATLAB 도움말 을 번역했을 뿐이니, 정확한 내용은 MATLAB 강화학습 툴박스 를 참고하여 주세요. 1. Water Distribution System Scheduling Using Reinforcement Learning ​ 이번 예제에서는 어떻게 강화학습(Reinfor...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/reinforce1/" class="btn btn-outline-primary" prompt="Older"><p>Reinforcement Learning, 강화학습이란 무엇인가</p></a> <a href="/posts/NoticeToStudent/" class="btn btn-outline-primary" prompt="Newer"><p>Notice, 정리하는 시간</p></a></div></div></div></div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script> <script type="text/javascript"> const imgs = document.querySelectorAll('.post-content img'); const observer = lozad(imgs); observer.observe(); </script><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2021 <a href="https://github.com/simonwithwoogi">Simon Anderson</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/matlab/">MATLAB</a> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/java/">java</a> <a class="post-tag" href="/tags/algorithm/">Algorithm</a> <a class="post-tag" href="/tags/deeplearning/">DeepLearning</a> <a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/statistics/">Statistics</a> <a class="post-tag" href="/tags/bigdata/">BigData</a> <a class="post-tag" href="/tags/improvement/">Improvement</a> <a class="post-tag" href="/tags/optimization/">Optimization</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://SimonWithWoogi.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>
